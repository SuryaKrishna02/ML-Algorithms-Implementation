{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a DNN with 20 hidden layers of 100 neurons each using He intialisation and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation = \"elu\",\n",
    "                                 kernel_initializer = \"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the softmax layers as the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the CIFAR10 dataset. And use first 5000 images of training set as validation set for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the callbacks and use them in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_model.h5\", save_best_only = True)\n",
    "run_index = 1 #In order to make sure to create different log folder for tensorboard\n",
    "run_logdir = os.path.join(os.curdir, \"cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8080 (pid 11832), started 0:00:01 ago. (Use '!kill 11832' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aa932ef57473151\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aa932ef57473151\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8080;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./cifar10_logs --port=8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/1407 [..............................] - ETA: 0s - loss: 165.9659 - accuracy: 0.0625WARNING:tensorflow:From C:\\Users\\G Surya Krishna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1407 [..............................] - ETA: 56s - loss: 133.4793 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0708s). Check your callbacks.\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 4.0140 - accuracy: 0.1670 - val_loss: 2.2015 - val_accuracy: 0.2172\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 2.0935 - accuracy: 0.2375 - val_loss: 2.0857 - val_accuracy: 0.2390\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.9586 - accuracy: 0.2818 - val_loss: 2.0965 - val_accuracy: 0.2520\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.8704 - accuracy: 0.3200 - val_loss: 1.8752 - val_accuracy: 0.3266\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.8030 - accuracy: 0.3443 - val_loss: 1.8373 - val_accuracy: 0.3272\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.7522 - accuracy: 0.3657 - val_loss: 1.7403 - val_accuracy: 0.3702\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.7075 - accuracy: 0.3816 - val_loss: 1.6955 - val_accuracy: 0.3790\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6701 - accuracy: 0.3988 - val_loss: 1.6713 - val_accuracy: 0.3906\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6335 - accuracy: 0.4100 - val_loss: 1.6621 - val_accuracy: 0.4086\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6092 - accuracy: 0.4192 - val_loss: 1.6948 - val_accuracy: 0.3934\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5841 - accuracy: 0.4278 - val_loss: 1.6707 - val_accuracy: 0.4012\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5614 - accuracy: 0.4377 - val_loss: 1.6570 - val_accuracy: 0.4012\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5425 - accuracy: 0.4420 - val_loss: 1.6383 - val_accuracy: 0.4166\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5287 - accuracy: 0.4495 - val_loss: 1.6072 - val_accuracy: 0.4264\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5130 - accuracy: 0.4539 - val_loss: 1.5975 - val_accuracy: 0.4260\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4947 - accuracy: 0.4635 - val_loss: 1.5677 - val_accuracy: 0.4312\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4833 - accuracy: 0.4676 - val_loss: 1.5841 - val_accuracy: 0.4356\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4702 - accuracy: 0.4705 - val_loss: 1.5963 - val_accuracy: 0.4250\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4539 - accuracy: 0.4760 - val_loss: 1.5587 - val_accuracy: 0.4410\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4429 - accuracy: 0.4836 - val_loss: 1.5380 - val_accuracy: 0.4546\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4339 - accuracy: 0.4826 - val_loss: 1.5914 - val_accuracy: 0.4386\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4217 - accuracy: 0.4873 - val_loss: 1.5518 - val_accuracy: 0.4436\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4104 - accuracy: 0.4930 - val_loss: 1.5643 - val_accuracy: 0.4296\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3988 - accuracy: 0.4969 - val_loss: 1.5778 - val_accuracy: 0.4336\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3900 - accuracy: 0.5010 - val_loss: 1.5029 - val_accuracy: 0.4652\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3783 - accuracy: 0.5047 - val_loss: 1.5273 - val_accuracy: 0.4556\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3657 - accuracy: 0.5092 - val_loss: 1.5078 - val_accuracy: 0.4610\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3592 - accuracy: 0.5141 - val_loss: 1.5091 - val_accuracy: 0.4658\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3525 - accuracy: 0.5146 - val_loss: 1.5000 - val_accuracy: 0.4670\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3392 - accuracy: 0.5164 - val_loss: 1.5912 - val_accuracy: 0.4466\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3344 - accuracy: 0.5195 - val_loss: 1.5803 - val_accuracy: 0.4528\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3242 - accuracy: 0.5245 - val_loss: 1.5107 - val_accuracy: 0.4634\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3165 - accuracy: 0.5272 - val_loss: 1.5614 - val_accuracy: 0.4556\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3063 - accuracy: 0.5304 - val_loss: 1.5300 - val_accuracy: 0.4706\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2991 - accuracy: 0.5330 - val_loss: 1.5120 - val_accuracy: 0.4736\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2932 - accuracy: 0.5356 - val_loss: 1.5286 - val_accuracy: 0.4612\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2786 - accuracy: 0.5415 - val_loss: 1.5186 - val_accuracy: 0.4664\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2712 - accuracy: 0.5431 - val_loss: 1.5203 - val_accuracy: 0.4746\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2669 - accuracy: 0.5433 - val_loss: 1.5355 - val_accuracy: 0.4636\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2556 - accuracy: 0.5492 - val_loss: 1.5200 - val_accuracy: 0.4716\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2470 - accuracy: 0.5516 - val_loss: 1.5239 - val_accuracy: 0.4668\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2440 - accuracy: 0.5533 - val_loss: 1.5387 - val_accuracy: 0.4724\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2354 - accuracy: 0.5576 - val_loss: 1.5755 - val_accuracy: 0.4594\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2259 - accuracy: 0.5620 - val_loss: 1.6026 - val_accuracy: 0.4508\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2189 - accuracy: 0.5617 - val_loss: 1.5117 - val_accuracy: 0.4768\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2111 - accuracy: 0.5657 - val_loss: 1.5098 - val_accuracy: 0.4752\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2074 - accuracy: 0.5654 - val_loss: 1.5377 - val_accuracy: 0.4716\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1982 - accuracy: 0.5675 - val_loss: 1.4934 - val_accuracy: 0.4916\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1892 - accuracy: 0.5729 - val_loss: 1.5535 - val_accuracy: 0.4668\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1835 - accuracy: 0.5740 - val_loss: 1.5519 - val_accuracy: 0.4722\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1771 - accuracy: 0.5774 - val_loss: 1.5502 - val_accuracy: 0.4732\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1681 - accuracy: 0.5810 - val_loss: 1.5611 - val_accuracy: 0.4612\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1595 - accuracy: 0.5836 - val_loss: 1.5471 - val_accuracy: 0.4716\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1555 - accuracy: 0.5850 - val_loss: 1.5758 - val_accuracy: 0.4684\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1477 - accuracy: 0.5866 - val_loss: 1.5684 - val_accuracy: 0.4694\n",
      "Epoch 56/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1435 - accuracy: 0.5884 - val_loss: 1.5305 - val_accuracy: 0.4834\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1319 - accuracy: 0.5921 - val_loss: 1.5520 - val_accuracy: 0.4732\n",
      "Epoch 58/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1330 - accuracy: 0.5929 - val_loss: 1.5621 - val_accuracy: 0.4668\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1248 - accuracy: 0.5966 - val_loss: 1.5720 - val_accuracy: 0.4840\n",
      "Epoch 60/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1163 - accuracy: 0.6000 - val_loss: 1.5733 - val_accuracy: 0.4796\n",
      "Epoch 61/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1107 - accuracy: 0.6016 - val_loss: 1.5747 - val_accuracy: 0.4798\n",
      "Epoch 62/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1064 - accuracy: 0.6026 - val_loss: 1.5798 - val_accuracy: 0.4776\n",
      "Epoch 63/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0981 - accuracy: 0.6053 - val_loss: 1.5549 - val_accuracy: 0.4796\n",
      "Epoch 64/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0897 - accuracy: 0.6104 - val_loss: 1.6082 - val_accuracy: 0.4720\n",
      "Epoch 65/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0882 - accuracy: 0.6097 - val_loss: 1.6194 - val_accuracy: 0.4678\n",
      "Epoch 66/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0771 - accuracy: 0.6126 - val_loss: 1.5951 - val_accuracy: 0.4664\n",
      "Epoch 67/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0751 - accuracy: 0.6133 - val_loss: 1.5926 - val_accuracy: 0.4734\n",
      "Epoch 68/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0709 - accuracy: 0.6149 - val_loss: 1.6168 - val_accuracy: 0.4752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d8cc4f1c70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 100, \n",
    "          validation_data = (X_valid, y_valid),\n",
    "         callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see with lowest validation loss we get 47% accuracy on the validation set.It took 39(59-20) epochs to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add Batch Normalization layers after each layer except the output layers and before the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/1407 [..............................] - ETA: 0s - loss: 3.0445 - accuracy: 0.1250WARNING:tensorflow:From C:\\Users\\G Surya Krishna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1407 [..............................] - ETA: 1:35 - loss: 2.8693 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.1244s). Check your callbacks.\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.8413 - accuracy: 0.3410 - val_loss: 1.6651 - val_accuracy: 0.4082\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.6683 - accuracy: 0.4052 - val_loss: 1.6210 - val_accuracy: 0.4146\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5970 - accuracy: 0.4328 - val_loss: 1.5325 - val_accuracy: 0.4412\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5477 - accuracy: 0.4483 - val_loss: 1.4931 - val_accuracy: 0.4660\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.5044 - accuracy: 0.4640 - val_loss: 1.4510 - val_accuracy: 0.4826\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4679 - accuracy: 0.4788 - val_loss: 1.4246 - val_accuracy: 0.4886\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4333 - accuracy: 0.4918 - val_loss: 1.4219 - val_accuracy: 0.4906\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4069 - accuracy: 0.5016 - val_loss: 1.3982 - val_accuracy: 0.5026\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3832 - accuracy: 0.5106 - val_loss: 1.3673 - val_accuracy: 0.5084\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3588 - accuracy: 0.5166 - val_loss: 1.3804 - val_accuracy: 0.5128\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3393 - accuracy: 0.5235 - val_loss: 1.3537 - val_accuracy: 0.5222\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3207 - accuracy: 0.5335 - val_loss: 1.3791 - val_accuracy: 0.5024\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2992 - accuracy: 0.5390 - val_loss: 1.3880 - val_accuracy: 0.5174\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2840 - accuracy: 0.5453 - val_loss: 1.3404 - val_accuracy: 0.5318\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2613 - accuracy: 0.5522 - val_loss: 1.3315 - val_accuracy: 0.5324\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2514 - accuracy: 0.5585 - val_loss: 1.3731 - val_accuracy: 0.5248\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2330 - accuracy: 0.5620 - val_loss: 1.3208 - val_accuracy: 0.5384\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2176 - accuracy: 0.5688 - val_loss: 1.3219 - val_accuracy: 0.5392\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2002 - accuracy: 0.5752 - val_loss: 1.3170 - val_accuracy: 0.5366\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1901 - accuracy: 0.5806 - val_loss: 1.3631 - val_accuracy: 0.5272\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1776 - accuracy: 0.5848 - val_loss: 1.3645 - val_accuracy: 0.5264\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1623 - accuracy: 0.5883 - val_loss: 1.3379 - val_accuracy: 0.5252\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1522 - accuracy: 0.5941 - val_loss: 1.3159 - val_accuracy: 0.5438\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1354 - accuracy: 0.6012 - val_loss: 1.3151 - val_accuracy: 0.5414\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1229 - accuracy: 0.6034 - val_loss: 1.3134 - val_accuracy: 0.5506\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1108 - accuracy: 0.6069 - val_loss: 1.3556 - val_accuracy: 0.5332\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1016 - accuracy: 0.6107 - val_loss: 1.3345 - val_accuracy: 0.5404\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0966 - accuracy: 0.6117 - val_loss: 1.3346 - val_accuracy: 0.5388\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0855 - accuracy: 0.6186 - val_loss: 1.3351 - val_accuracy: 0.5388\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0706 - accuracy: 0.6229 - val_loss: 1.3616 - val_accuracy: 0.5354\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0589 - accuracy: 0.6280 - val_loss: 1.3549 - val_accuracy: 0.5380\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0506 - accuracy: 0.6287 - val_loss: 1.3688 - val_accuracy: 0.5408\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0339 - accuracy: 0.6356 - val_loss: 1.3433 - val_accuracy: 0.5488\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0325 - accuracy: 0.6378 - val_loss: 1.3463 - val_accuracy: 0.5432\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0168 - accuracy: 0.6417 - val_loss: 1.3691 - val_accuracy: 0.5358\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0152 - accuracy: 0.6440 - val_loss: 1.3581 - val_accuracy: 0.5414\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9991 - accuracy: 0.6471 - val_loss: 1.3426 - val_accuracy: 0.5448\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9917 - accuracy: 0.6506 - val_loss: 1.3758 - val_accuracy: 0.5484\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.9820 - accuracy: 0.6522 - val_loss: 1.3845 - val_accuracy: 0.5386\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9715 - accuracy: 0.6590 - val_loss: 1.3848 - val_accuracy: 0.5398\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9670 - accuracy: 0.6593 - val_loss: 1.3658 - val_accuracy: 0.5488\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.9577 - accuracy: 0.6629 - val_loss: 1.3953 - val_accuracy: 0.5414\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.9506 - accuracy: 0.6650 - val_loss: 1.3821 - val_accuracy: 0.5400\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.9417 - accuracy: 0.6678 - val_loss: 1.3915 - val_accuracy: 0.5350\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 0.9308 - accuracy: 0.6733 - val_loss: 1.4126 - val_accuracy: 0.5320\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.3134 - accuracy: 0.0920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.31340754032135, 0.09200000017881393]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer = \"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4) #Note the change in learning rate.\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_bn_model.h5\", save_best_only = True)\n",
    "run_index = 1 #In order to make sure to create different log folder for tensorboard\n",
    "run_logdir = os.path.join(os.curdir, \"cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 100, \n",
    "          validation_data = (X_valid, y_valid),\n",
    "         callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model is converging faster, producing better model and Training speed is decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELU as Activation (Self-Normalized Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about replacing Batch Normalization with SELU? Let's do it by ensuring the network is self-normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 59:34 - loss: 3.0440 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 5.0807s). Check your callbacks.\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.9331 - accuracy: 0.3072 - val_loss: 1.8533 - val_accuracy: 0.3268\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7170 - accuracy: 0.3907 - val_loss: 1.8202 - val_accuracy: 0.3390\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6181 - accuracy: 0.4279 - val_loss: 1.6897 - val_accuracy: 0.3962\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5512 - accuracy: 0.4548 - val_loss: 1.6436 - val_accuracy: 0.4336\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4913 - accuracy: 0.4808 - val_loss: 1.5915 - val_accuracy: 0.4298\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4422 - accuracy: 0.4958 - val_loss: 1.5479 - val_accuracy: 0.4506\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3932 - accuracy: 0.5126 - val_loss: 1.5112 - val_accuracy: 0.4596\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3593 - accuracy: 0.5241 - val_loss: 1.4859 - val_accuracy: 0.4736\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3194 - accuracy: 0.5406 - val_loss: 1.5037 - val_accuracy: 0.4700\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2862 - accuracy: 0.5535 - val_loss: 1.4595 - val_accuracy: 0.5046\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2581 - accuracy: 0.5639 - val_loss: 1.4859 - val_accuracy: 0.4956\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2278 - accuracy: 0.5757 - val_loss: 1.4697 - val_accuracy: 0.4982\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1997 - accuracy: 0.5883 - val_loss: 1.5097 - val_accuracy: 0.5058\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1840 - accuracy: 0.5941 - val_loss: 1.4605 - val_accuracy: 0.5010\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1572 - accuracy: 0.6039 - val_loss: 1.5037 - val_accuracy: 0.4970\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1355 - accuracy: 0.6113 - val_loss: 1.5403 - val_accuracy: 0.5016\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1103 - accuracy: 0.6226 - val_loss: 1.5282 - val_accuracy: 0.5140\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0952 - accuracy: 0.6272 - val_loss: 1.4591 - val_accuracy: 0.5072\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0664 - accuracy: 0.6386 - val_loss: 1.5171 - val_accuracy: 0.5148\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0525 - accuracy: 0.6428 - val_loss: 1.5109 - val_accuracy: 0.5020\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0409 - accuracy: 0.6485 - val_loss: 1.5317 - val_accuracy: 0.5182\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0201 - accuracy: 0.6552 - val_loss: 1.5191 - val_accuracy: 0.5058\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9987 - accuracy: 0.6602 - val_loss: 1.5916 - val_accuracy: 0.4974\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9817 - accuracy: 0.6702 - val_loss: 1.5705 - val_accuracy: 0.5146\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9764 - accuracy: 0.6722 - val_loss: 1.5846 - val_accuracy: 0.5142\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 22.6227 - accuracy: 0.6216 - val_loss: 1.7020 - val_accuracy: 0.4226\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2607 - accuracy: 0.5645 - val_loss: 1.5736 - val_accuracy: 0.4768\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 1.1735 - accuracy: 0.5961 - val_loss: 1.5280 - val_accuracy: 0.4874\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1192 - accuracy: 0.6135 - val_loss: 1.5251 - val_accuracy: 0.5012\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 1.0782 - accuracy: 0.6279 - val_loss: 1.5714 - val_accuracy: 0.5010\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0454 - accuracy: 0.6401 - val_loss: 1.5315 - val_accuracy: 0.5062\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0218 - accuracy: 0.6480 - val_loss: 1.5562 - val_accuracy: 0.5082\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0058 - accuracy: 0.6558 - val_loss: 1.5520 - val_accuracy: 0.5028\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9816 - accuracy: 0.6650 - val_loss: 1.6076 - val_accuracy: 0.5044\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0255 - accuracy: 0.6515 - val_loss: 1.5843 - val_accuracy: 0.5046\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9881 - accuracy: 0.6635 - val_loss: 1.5715 - val_accuracy: 0.5094\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9387 - accuracy: 0.6799 - val_loss: 1.6160 - val_accuracy: 0.5002\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 17.7853 - accuracy: 0.6238 - val_loss: 1.5694 - val_accuracy: 0.4982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18cb0760b50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, \n",
    "                                 activation = \"selu\",\n",
    "                                 kernel_initializer = \"lecun_normal\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=7e-4) #Note the change in learning rate.\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 \n",
    "run_logdir = os.path.join(os.curdir, \"cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis = 0)\n",
    "X_stds = X_train.std(axis = 0)\n",
    "X_train_scaled = (X_train-X_means)/X_stds\n",
    "X_valid_scaled = (X_valid-X_means)/X_stds\n",
    "X_test_scaled = (X_test - X_means)/X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the better accuracy than the original model, but not as good as using batch normalisation.\n",
    "It takes much less epochs to converge(18 epochs) which makes it faster model that we trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try regularizing the model with aplha dropout. Then, without retraining the model, see if MC Dropout can achieve better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 40:03 - loss: 2.9857 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 3.4109s). Check your callbacks.\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.8891 - accuracy: 0.3298 - val_loss: 1.6898 - val_accuracy: 0.4044\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6603 - accuracy: 0.4161 - val_loss: 1.6790 - val_accuracy: 0.4034\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5726 - accuracy: 0.4475 - val_loss: 1.6599 - val_accuracy: 0.4176\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5062 - accuracy: 0.4709 - val_loss: 1.6294 - val_accuracy: 0.4416\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4505 - accuracy: 0.4938 - val_loss: 1.5735 - val_accuracy: 0.4616\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4053 - accuracy: 0.5122 - val_loss: 1.5561 - val_accuracy: 0.4744\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.3624 - accuracy: 0.5254 - val_loss: 1.5537 - val_accuracy: 0.4770\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3281 - accuracy: 0.5396 - val_loss: 1.5155 - val_accuracy: 0.4960\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2932 - accuracy: 0.5514 - val_loss: 1.5191 - val_accuracy: 0.4810\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2605 - accuracy: 0.5636 - val_loss: 1.5159 - val_accuracy: 0.4916\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.2286 - accuracy: 0.5746 - val_loss: 1.5590 - val_accuracy: 0.4922\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1992 - accuracy: 0.5868 - val_loss: 1.5087 - val_accuracy: 0.5086\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1692 - accuracy: 0.5985 - val_loss: 1.5920 - val_accuracy: 0.4960\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1461 - accuracy: 0.6056 - val_loss: 1.5891 - val_accuracy: 0.5042\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1187 - accuracy: 0.6144 - val_loss: 1.6215 - val_accuracy: 0.5032\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0879 - accuracy: 0.6282 - val_loss: 1.6608 - val_accuracy: 0.5162\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0710 - accuracy: 0.6352 - val_loss: 1.6599 - val_accuracy: 0.5030\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0556 - accuracy: 0.6420 - val_loss: 1.6587 - val_accuracy: 0.5148\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0296 - accuracy: 0.6508 - val_loss: 1.7094 - val_accuracy: 0.5142\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0105 - accuracy: 0.6561 - val_loss: 1.7196 - val_accuracy: 0.5038\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9891 - accuracy: 0.6640 - val_loss: 1.7102 - val_accuracy: 0.5132\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9696 - accuracy: 0.6689 - val_loss: 1.6707 - val_accuracy: 0.4996\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9452 - accuracy: 0.6777 - val_loss: 1.6968 - val_accuracy: 0.4956\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9319 - accuracy: 0.6844 - val_loss: 1.7382 - val_accuracy: 0.5144\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9174 - accuracy: 0.6910 - val_loss: 1.7977 - val_accuracy: 0.5128\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8976 - accuracy: 0.6963 - val_loss: 1.8284 - val_accuracy: 0.5048\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8813 - accuracy: 0.7033 - val_loss: 1.7796 - val_accuracy: 0.5216\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8739 - accuracy: 0.7060 - val_loss: 1.7649 - val_accuracy: 0.5120\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.8507 - accuracy: 0.7116 - val_loss: 1.9092 - val_accuracy: 0.5088\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8380 - accuracy: 0.7209 - val_loss: 1.8758 - val_accuracy: 0.5024\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8263 - accuracy: 0.7238 - val_loss: 1.7623 - val_accuracy: 0.5002\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8113 - accuracy: 0.7272 - val_loss: 1.9174 - val_accuracy: 0.5060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18c9619d6a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, \n",
    "                                 activation = \"selu\",\n",
    "                                 kernel_initializer = \"lecun_normal\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate = 0.1))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4) #Note the change in learning rate.\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_alpha_model.h5\", save_best_only=True)\n",
    "run_index = 1 \n",
    "run_logdir = os.path.join(os.curdir, \"cifar10_logs\", \"run_alpha_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis = 0)\n",
    "X_stds = X_train.std(axis = 0)\n",
    "X_train_scaled = (X_train-X_means)/X_stds\n",
    "X_valid_scaled = (X_valid-X_means)/X_stds\n",
    "X_test_scaled = (X_test - X_means)/X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy is little spoiled compared to previous model but the convergence occured in 12 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout. In order to use it we have to run 10 times the same model with dropout layers during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self,inputs):\n",
    "        return super().call(inputs, training = True) # In order use dropout layers at the inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new model with same weights as previous model, but with **MCAlphaDropout** instead **AlphaDropout**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer for layer in model.layers\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples = 10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis = 0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples = 10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5072"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:,0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no improvement in this case. So, the best model we got is Batch Normalisatioin Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1cycle Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, \n",
    "                                 activation = \"selu\",\n",
    "                                 kernel_initializer = \"lecun_normal\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate = 0.1))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=1e-4) #Note the change in learning rate.\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "#This callback is to reduce the learning rate after each iteration by the factor.\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "#This is just to find the learning rate for which the loss starts to increase,\n",
    "#Which is nothing but maximum learning rate in the 1cycle scheduler.\n",
    "#Note:- After runnning for one epoch, the model resettted to previous state.\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = len(X) // batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "#Plots the learning rates vs Losses inorder to see the max learning rate.\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 4s 10ms/step - loss: 2615682864775168.0000 - accuracy: 0.1419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.999868392944336,\n",
       " 2.4138946533203125,\n",
       " 3.864195857729231)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs = 1, batch_size = batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0]+min(losses))/1.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we can see at the learning rate 3e-3 the loss started diverging. \n",
    "So, this will be our maximum learning rate in onecycle scheduling.\n",
    "The Default learning rate should be 2e-3. But Generally, Optimal learning rate should is 10 times less than maximum learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [32,32,3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, \n",
    "                                 activation = \"selu\",\n",
    "                                 kernel_initializer = \"lecun_normal\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate = 0.1))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=2e-3) #Note the change in learning rate.\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = optimizer,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During first half of the training, the learning rate starts to increase from $\\eta_0$(max_rate/10) to $\\eta_1$ (max_rate) and during second half of the training it decreases linearly from $\\eta_1$ to $\\eta_0$. And in few final iterations the learning rate decreases linearly by several orders of magnitude.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 1.9298 - accuracy: 0.3256 - val_loss: 1.7120 - val_accuracy: 0.4064\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.6609 - accuracy: 0.4138 - val_loss: 1.6561 - val_accuracy: 0.4224\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.6078 - accuracy: 0.4388 - val_loss: 1.6568 - val_accuracy: 0.4226\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.5801 - accuracy: 0.4491 - val_loss: 1.7252 - val_accuracy: 0.4196\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.5831 - accuracy: 0.4513 - val_loss: 1.7650 - val_accuracy: 0.4264\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 1.5755 - accuracy: 0.4552 - val_loss: 1.6341 - val_accuracy: 0.4340\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 1.5937 - accuracy: 0.4501 - val_loss: 1.7122 - val_accuracy: 0.4136\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.5455 - accuracy: 0.4661 - val_loss: 1.6685 - val_accuracy: 0.4640\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.4693 - accuracy: 0.4899 - val_loss: 1.6053 - val_accuracy: 0.4582\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.4017 - accuracy: 0.5167 - val_loss: 1.6017 - val_accuracy: 0.4764\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.3317 - accuracy: 0.5425 - val_loss: 1.5173 - val_accuracy: 0.4910\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.2536 - accuracy: 0.5674 - val_loss: 1.4575 - val_accuracy: 0.5040\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.1726 - accuracy: 0.5926 - val_loss: 1.4914 - val_accuracy: 0.5124\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.0858 - accuracy: 0.6235 - val_loss: 1.4771 - val_accuracy: 0.5228\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.0238 - accuracy: 0.6453 - val_loss: 1.5173 - val_accuracy: 0.5246\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled)//batch_size * n_epochs, max_rate = 3e-3)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to one cycle we just trained for 15 epochs but 3 times faster than the faster model we trained so far. Moreover, The Model performance imporoved drastically from 50% to 64% surpassing the previous best model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
