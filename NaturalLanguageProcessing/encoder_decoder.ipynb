{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "    \n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "    \n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364878"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date(1000, 1, 1).toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3652059"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date(9999, 12, 31).toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\"*50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the list of all possible characters in the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADFJMNOSabceghilmnoprstuvy0123456789, '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"0123456789, \"\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the list of possible characters in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 11, 19, 22, 11, 16, 9, 11, 20, 37, 28, 26, 36, 37, 33, 26, 33, 31]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor()  # using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Version: a very basic seq2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
    "\n",
    "Since the decoder expects a sequence as input, we repeat the vector (which is output by the decoder) as many times as the longest possible output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8238 - accuracy: 0.3444 - val_loss: 1.3930 - val_accuracy: 0.4723\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 1.3433 - accuracy: 0.5123 - val_loss: 1.2283 - val_accuracy: 0.5559\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.0633 - accuracy: 0.6173 - val_loss: 0.9442 - val_accuracy: 0.6590\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.8061 - accuracy: 0.7016 - val_loss: 0.8413 - val_accuracy: 0.6798\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6912 - accuracy: 0.7490 - val_loss: 0.5185 - val_accuracy: 0.7995\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.5344 - accuracy: 0.8026 - val_loss: 0.4014 - val_accuracy: 0.8424\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3334 - accuracy: 0.8748 - val_loss: 0.2732 - val_accuracy: 0.9011\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6199 - accuracy: 0.7876 - val_loss: 0.6008 - val_accuracy: 0.7723\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2783 - accuracy: 0.9103 - val_loss: 0.1530 - val_accuracy: 0.9585\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.1158 - accuracy: 0.9725 - val_loss: 0.0985 - val_accuracy: 0.9764\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0663 - accuracy: 0.9873 - val_loss: 0.0553 - val_accuracy: 0.9904\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0411 - accuracy: 0.9942 - val_loss: 0.0345 - val_accuracy: 0.9951\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2251 - accuracy: 0.9474 - val_loss: 0.0868 - val_accuracy: 0.9875\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0485 - accuracy: 0.9945 - val_loss: 0.0337 - val_accuracy: 0.9963\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0234 - accuracy: 0.9984 - val_loss: 0.0202 - val_accuracy: 0.9986\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0147 - accuracy: 0.9993 - val_loss: 0.0137 - val_accuracy: 0.9992\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0100 - accuracy: 0.9997 - val_loss: 0.0100 - val_accuracy: 0.9997\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0071 - accuracy: 0.9999 - val_loss: 0.0073 - val_accuracy: 0.9998\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0052 - accuracy: 0.9999 - val_loss: 0.0055 - val_accuracy: 0.9998\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim = len(INPUT_CHARS) +1,\n",
    "                           output_dim = embedding_size,\n",
    "                           input_shape = [None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "             metrics = [\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, epochs = 20,\n",
    "                   validation_data = (X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
    "           for sequence in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-472ea7c41409>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! :)\n",
    "\n",
    "However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-02\n",
      "1789-09-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that we always pass sequences of the same length as during training, using padding if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        #[[top, bottom],[left, right]]\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = model.predict_classes(X)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second version: feeding the shifted targets to the decoder (teacher forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.\n",
    "\n",
    "Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n",
    "\n",
    "During inference, we won't know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n",
    "\n",
    "But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass the vector output by the encoder? Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n",
    "\n",
    "Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims =(len(Y), 1), value = sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis = 1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.6848 - accuracy: 0.3746 - val_loss: 1.4282 - val_accuracy: 0.4592\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 1.1913 - accuracy: 0.5624 - val_loss: 0.9003 - val_accuracy: 0.6798\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.5973 - accuracy: 0.7932 - val_loss: 0.3314 - val_accuracy: 0.9041\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2083 - accuracy: 0.9529 - val_loss: 0.0916 - val_accuracy: 0.9906\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0688 - accuracy: 0.9930 - val_loss: 0.0383 - val_accuracy: 0.9987\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0535 - accuracy: 0.9934 - val_loss: 0.0240 - val_accuracy: 0.9997\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0162 - accuracy: 0.9999 - val_loss: 0.0132 - val_accuracy: 0.9998\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim =len(INPUT_CHARS) + 1,\n",
    "    output_dim = encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
    "    lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2,\n",
    "    output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences = True)(\n",
    "    decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) +1,\n",
    "                                   activation = \"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input],\n",
    "                          outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs = 10,\n",
    "                   validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value = sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis = -1, output_type = tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis = 1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third version: using TF-Addons's seq2seq implementation\n",
    "\n",
    "Let's build exactly the same model, but using TF-Addon's seq2seq API. The implementation below is almost very similar to the TFA example higher in this notebook, except without the model input to specify the output sequence length, for simplicity (but you can easily add it back in if you need it for your projects, when the output sequences have very different lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.6793 - accuracy: 0.3668 - val_loss: 1.4547 - val_accuracy: 0.4364\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 1.3875 - accuracy: 0.4571 - val_loss: 1.3003 - val_accuracy: 0.4942\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.0189 - accuracy: 0.6279 - val_loss: 0.7626 - val_accuracy: 0.7342\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.4980 - accuracy: 0.8302 - val_loss: 0.2509 - val_accuracy: 0.9377\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.1789 - accuracy: 0.9641 - val_loss: 0.0840 - val_accuracy: 0.9920\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0549 - accuracy: 0.9967 - val_loss: 0.0378 - val_accuracy: 0.9984\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0491 - accuracy: 0.9939 - val_loss: 0.0259 - val_accuracy: 0.9991\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.0171 - accuracy: 0.9998 - val_loss: 0.0159 - val_accuracy: 0.9993\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9999\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 0.9999\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9999\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape = [None], dtype = np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape = [None], dtype = np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                sampler,\n",
    "                                                output_layer=output_layer)\n",
    "\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state = encoder_state)\n",
    "\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs,  decoder_inputs],\n",
    "                              outputs=[Y_proba])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "             metrics = [\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
    "                   validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there's a much more efficient way to perform inference. Until now, during inference, we've run the model once for each new character. Instead, we can create a new decoder, based on the previously trained layers, but using a `GreedyEmbeddingSampler` instead of a `TrainingSampler`.\n",
    "\n",
    "At each time step, the `GreedyEmbeddingSampler` will compute the argmax of the decoder's outputs, and run the resulting token IDs through the decoder's embedding layer. Then it will feed the resulting embeddings to the decoder's LSTM cell at the next time step. This way, we only need to run the decoder once to get the full prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "    embedding_fn = decoder_embedding_layer)\n",
    "\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=max_output_length)\n",
    "\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
    "                                    outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes:\n",
    "* The `GreedyEmbeddingSampler` needs the `start_tokens` (a vector containing the start-of-sequence ID for each decoder sequence), and the `end_token` (the decoder will stop decoding a sequence once the model outputs this token).\n",
    "* We must set `maximum_iterations` when creating the `BasicDecoder`, or else it may run into an infinite loop (if the model never outputs the end token for at least one of the sequences). This would force you would to restart the Jupyter kernel.\n",
    "* The decoder inputs are not needed anymore, since all the decoder inputs are generated dynamically based on the outputs from the previous time step.\n",
    "* The model's outputs are `final_outputs.sample_id` instead of the softmax of `final_outputs.rnn_outputs`. This allows us to directly get the argmax of the model's outputs. If you prefer to have access to the logits, you can replace `final_outputs.sample_id` with `final_outputs.rnn_outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 ms ± 16.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 ms ± 2.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth version: using TF-Addons's seq2seq implementation with a scheduled sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we trained the previous model, at each time step _t_ we gave the model the target token for time step _t_ - 1. However, at inference time, the model did not get the previous target at each time step. Instead, it got the previous prediction. So there is a discrepancy between training and inference, which may lead to disappointing performance. To alleviate this, we can gradually replace the targets with the predictions, during training. For this, we just need to replace the `TrainingSampler` with a `ScheduledEmbeddingTrainingSampler`, and use a Keras callback to gradually increase the `sampling_probability` (i.e., the probability that the decoder will use the prediction from the previous time step rather than the target for the previous time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G Surya Krishna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 17s 53ms/step - loss: 1.6793 - accuracy: 0.3668 - val_loss: 1.4547 - val_accuracy: 0.4366\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 1.3882 - accuracy: 0.4603 - val_loss: 1.2549 - val_accuracy: 0.5329\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.0127 - accuracy: 0.6265 - val_loss: 0.7641 - val_accuracy: 0.7214\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 0.5738 - accuracy: 0.7882 - val_loss: 0.3952 - val_accuracy: 0.8627\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.3526 - accuracy: 0.8887 - val_loss: 0.2433 - val_accuracy: 0.9267\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 0.1939 - accuracy: 0.9459 - val_loss: 0.1391 - val_accuracy: 0.9628\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.1142 - accuracy: 0.9720 - val_loss: 0.0945 - val_accuracy: 0.9775\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 0.1284 - accuracy: 0.9719 - val_loss: 0.0711 - val_accuracy: 0.9851\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 0.0499 - accuracy: 0.9903 - val_loss: 0.0382 - val_accuracy: 0.9930\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 0.0303 - accuracy: 0.9949 - val_loss: 0.0252 - val_accuracy: 0.9959\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0214 - accuracy: 0.9965 - val_loss: 0.0206 - val_accuracy: 0.9965\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0135 - accuracy: 0.9981 - val_loss: 0.0134 - val_accuracy: 0.9978\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.1236 - accuracy: 0.9691 - val_loss: 0.0158 - val_accuracy: 0.9980\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0104 - accuracy: 0.9990 - val_loss: 0.0106 - val_accuracy: 0.9988\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0067 - accuracy: 0.9994 - val_loss: 0.0065 - val_accuracy: 0.9994\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0049 - accuracy: 0.9996 - val_loss: 0.0046 - val_accuracy: 0.9995\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.0039 - val_accuracy: 0.9998\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.0024 - val_accuracy: 0.9998\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0026 - val_accuracy: 0.9999\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 15s 50ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0020 - val_accuracy: 0.9997\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_epochs = 20\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape = [None], dtype = np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape = [None], dtype = np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(\n",
    "    sampling_probability=0.,\n",
    "    embedding_fn=decoder_embedding_layer)\n",
    "\n",
    "sampler.sampling_probability = tf.Variable(0.)\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                sampler,\n",
    "                                                output_layer=output_layer)\n",
    "\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state = encoder_state)\n",
    "\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs,  decoder_inputs],\n",
    "                              outputs=[Y_proba])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "def update_sampling_probability(epoch, logs):\n",
    "    proba = min(1.0, epoch / (n_epochs - 10))\n",
    "    sampler.sampling_probability.assign(proba)\n",
    "\n",
    "sampling_probability_cb = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=update_sampling_probability)\n",
    "    \n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=n_epochs,\n",
    "                   validation_data=([X_valid, X_valid_decoder], Y_valid),\n",
    "                   callbacks=[sampling_probability_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference, we could do the exact same thing as earlier, using a `GreedyEmbeddingSampler`. However, just for the sake of completeness, let's use a `SampleEmbeddingSampler` instead. It's almost the same thing, except that instead of using the argmax of the model's output to find the token ID, it treats the outputs as logits and uses them to sample a token ID randomly. This can be useful when you want to generate text. The `softmax_temperature` argument serves the \n",
    "same purpose as when we generated Shakespeare-like text (the higher this argument, the more random the generated text will be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_temperature = tf.Variable(1.)\n",
    "\n",
    "inference_sampler = tfa.seq2seq.sampler.SampleEmbeddingSampler(\n",
    "    embedding_fn = decoder_embedding_layer,\n",
    "    softmax_temperature=softmax_temperature)\n",
    "\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=max_output_length)\n",
    "\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state = encoder_state,\n",
    "    start_tokens = start_tokens,\n",
    "    end_token = 0\n",
    ")\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
    "                                    outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creative_predict_date_strs(date_strs, temperature=1.0):\n",
    "    softmax_temperature.assign(temperature)\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1479307-19', '200040?400']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "creative_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"],\n",
    "                           temperature=5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth version: using TFA seq2seq, the Keras subclassing API and attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences in this problem are pretty short, but if we wanted to tackle longer sequences, we would probably have to use attention mechanisms. While it's possible to code our own implementation, it's simpler and more efficient to use TF-Addons's implementation instead. Let's do that now, this time using Keras' subclassing API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we've reverted back to using the `TrainingSampler`, for simplicity (but you can easily tweak it to use a `ScheduledEmbeddingTrainingSampler` instead). We also use a `GreedyEmbeddingSampler` during inference, so this class is pretty easy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTranslation(keras.models.Model):\n",
    "    def __init__(self, units=128, encoder_embedding_size=32,\n",
    "                decoder_embedding_size=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_embedding = keras.layers.Embedding(\n",
    "            input_dim = len(INPUT_CHARS) + 1,\n",
    "            output_dim = encoder_embedding_size)\n",
    "        self.encoder = keras.layers.LSTM(units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True)\n",
    "        self.decoder_embedding = keras.layers.Embedding(\n",
    "            input_dim = len(OUTPUT_CHARS) + 2,\n",
    "            output_dim = decoder_embedding_size)\n",
    "        self.attention = tfa.seq2seq.LuongAttention(units)\n",
    "        decoder_inner_cell = keras.layers.LSTMCell(units)\n",
    "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n",
    "            cell=decoder_inner_cell,\n",
    "            attention_mechanism=self.attention)\n",
    "        output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.TrainingSampler(),\n",
    "            output_layer=output_layer)\n",
    "        self.inference_decoder = tfa.seq2seq.BasicDecoder(\n",
    "            cell=self.decoder_cell,\n",
    "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "                embedding_fn = self.decoder_embedding),\n",
    "            output_layer=output_layer,\n",
    "            maximum_iterations=max_output_length)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        encoder_input, decoder_input = inputs\n",
    "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = self.encoder(\n",
    "            encoder_embeddings,\n",
    "            training=training)\n",
    "        encoder_state = [encoder_state_h, encoder_state_c]\n",
    "        \n",
    "        self.attention(encoder_outputs, setup_memory=True)\n",
    "        \n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
    "        \n",
    "        decoder_initial_state = self.decoder_cell.get_initial_state(\n",
    "            decoder_embeddings)\n",
    "        decoder_initial_state = decoder_initial_state.clone(\n",
    "            cell_state=encoder_state)\n",
    "        \n",
    "        if training:\n",
    "            decoder_outputs, _, _  = self.decoder(\n",
    "                decoder_embeddings,\n",
    "                initial_state = decoder_initial_state,\n",
    "                training=training)\n",
    "        else:\n",
    "            start_tokens = tf.zeros_like(encoder_input[:,0]) + sos_id\n",
    "            decoder_outputs, _, _ = self.inference_decoder(\n",
    "                decoder_embeddings,\n",
    "                initial_state=decoder_initial_state,\n",
    "                start_tokens=start_tokens,\n",
    "                end_token=0)\n",
    "        return tf.nn.softmax(decoder_outputs.rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 2.1425 - accuracy: 0.2302 - val_loss: 2.1138 - val_accuracy: 0.2379\n",
      "Epoch 2/25\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 1.8695 - accuracy: 0.3229 - val_loss: 2.1359 - val_accuracy: 0.2230\n",
      "Epoch 3/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.5347 - accuracy: 0.4513 - val_loss: 1.3687 - val_accuracy: 0.4842\n",
      "Epoch 4/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.3201 - accuracy: 0.5095 - val_loss: 1.2511 - val_accuracy: 0.5235\n",
      "Epoch 5/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.1755 - accuracy: 0.5569 - val_loss: 1.1243 - val_accuracy: 0.5739\n",
      "Epoch 6/25\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 1.0999 - accuracy: 0.5838 - val_loss: 1.1195 - val_accuracy: 0.5836\n",
      "Epoch 7/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.0446 - accuracy: 0.6056 - val_loss: 1.0075 - val_accuracy: 0.6169\n",
      "Epoch 8/25\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.9783 - accuracy: 0.6315 - val_loss: 0.9616 - val_accuracy: 0.6366\n",
      "Epoch 9/25\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.9605 - accuracy: 0.6359 - val_loss: 0.9317 - val_accuracy: 0.6452\n",
      "Epoch 10/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.9572 - accuracy: 0.6411 - val_loss: 0.9090 - val_accuracy: 0.6535\n",
      "Epoch 11/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.8807 - accuracy: 0.6657 - val_loss: 0.8537 - val_accuracy: 0.6779\n",
      "Epoch 12/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.8049 - accuracy: 0.6965 - val_loss: 0.8131 - val_accuracy: 0.7073 0.8081 - accuracy:  - ETA: 0s - loss: 0.8061 - accuracy\n",
      "Epoch 13/25\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 0.6334 - accuracy: 0.7474 - val_loss: 0.7343 - val_accuracy: 0.7453\n",
      "Epoch 14/25\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 0.5118 - accuracy: 0.8008 - val_loss: 0.5979 - val_accuracy: 0.7811\n",
      "Epoch 15/25\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 0.3317 - accuracy: 0.8616 - val_loss: 0.3661 - val_accuracy: 0.8694\n",
      "Epoch 16/25\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.2084 - accuracy: 0.9199 - val_loss: 0.3306 - val_accuracy: 0.9125\n",
      "Epoch 17/25\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.1700 - accuracy: 0.9529 - val_loss: 0.2368 - val_accuracy: 0.9469\n",
      "Epoch 18/25\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0967 - accuracy: 0.9824 - val_loss: 0.2190 - val_accuracy: 0.9694\n",
      "Epoch 19/25\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0578 - accuracy: 0.9930 - val_loss: 0.1555 - val_accuracy: 0.9730\n",
      "Epoch 20/25\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.0725 - accuracy: 0.9874 - val_loss: 0.0589 - val_accuracy: 0.9943\n",
      "Epoch 21/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.0233 - accuracy: 0.9991 - val_loss: 0.0237 - val_accuracy: 0.9987\n",
      "Epoch 22/25\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 0.0147 - accuracy: 0.9996 - val_loss: 0.0224 - val_accuracy: 0.9979\n",
      "Epoch 23/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.0949 - accuracy: 0.9797 - val_loss: 0.2278 - val_accuracy: 0.9442\n",
      "Epoch 24/25\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.0301 - accuracy: 0.9963 - val_loss: 0.0153 - val_accuracy: 0.9995\n",
      "Epoch 25/25\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 0.0099 - accuracy: 0.9999 - val_loss: 0.0085 - val_accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = DateTranslation()\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs = 25,\n",
    "                   validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs_v2(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    X_decoder = tf.zeros(shape=(len(X), max_output_length), dtype=tf.int32)\n",
    "    Y_probas = model.predict([X, X_decoder])\n",
    "    Y_predict = tf.argmax(Y_probas, axis = -1)\n",
    "    return ids_to_date_strs(Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs_v2([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
