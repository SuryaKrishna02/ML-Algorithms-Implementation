{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bach Chorales dataset composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note's index on a piano (except for the value 0, which means that no note is played)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\n",
    "FILENAME = \"jsb_chorales.tgz\"\n",
    "filepath = keras.utils.get_file(FILENAME,\n",
    "                                DOWNLOAD_ROOT + FILENAME,\n",
    "                                cache_subdir=\"datasets/jsb_chorales\",\n",
    "                                extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [74, 70, 65, 58],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 58, 55],\n",
       " [75, 70, 60, 55],\n",
       " [75, 70, 60, 55],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 69, 62, 50],\n",
       " [77, 70, 62, 55],\n",
       " [77, 70, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [77, 69, 62, 55],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [74, 70, 65, 46],\n",
       " [75, 69, 63, 48],\n",
       " [75, 69, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [75, 67, 63, 48],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 62, 50],\n",
       " [77, 65, 60, 50],\n",
       " [77, 65, 60, 50],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 53],\n",
       " [74, 67, 58, 53],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [72, 69, 65, 53],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [74, 71, 53, 50],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 48],\n",
       " [75, 72, 55, 50],\n",
       " [75, 72, 55, 50],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 51],\n",
       " [75, 67, 60, 53],\n",
       " [75, 67, 60, 53],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 60, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 67, 57, 55],\n",
       " [74, 65, 59, 43],\n",
       " [74, 65, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 59, 43],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [72, 63, 55, 48],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [75, 67, 60, 60],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 58],\n",
       " [77, 70, 62, 56],\n",
       " [77, 70, 62, 56],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 55],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 62, 53],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [79, 70, 63, 51],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 63, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 60, 58],\n",
       " [77, 70, 62, 46],\n",
       " [77, 70, 62, 46],\n",
       " [77, 68, 62, 46],\n",
       " [75, 68, 62, 46],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [74, 67, 58, 55],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 53],\n",
       " [75, 67, 58, 51],\n",
       " [75, 67, 58, 51],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 58, 50],\n",
       " [77, 65, 56, 50],\n",
       " [77, 65, 56, 50],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [70, 63, 55, 51],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [75, 65, 60, 45],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [74, 65, 58, 46],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 57],\n",
       " [75, 67, 58, 55],\n",
       " [75, 67, 58, 55],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 57],\n",
       " [77, 65, 60, 53],\n",
       " [77, 65, 60, 53],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [74, 65, 58, 58],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 67, 58, 51],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [72, 65, 57, 53],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46],\n",
       " [70, 65, 62, 46]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chorales[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes range from 36 (C1 = C on octave 1) to 81 (A5 = A on octave 5), plus 0 for silence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)  #Union\n",
    "            \n",
    "            \n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a few functions to listen to these chorales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    # Frequency doubles when you go up one octave; there are 12 semi-tones\n",
    "    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n",
    "    #To reduce click sound at every beat, we round the frequencies to try\n",
    "    # to get the samples close to zero at the end of each note.\n",
    "    \n",
    "    frequencies = np.round(note_duration * frequencies) / note_duration\n",
    "    n_samples = int(note_duration * sample_rate)\n",
    "    time = np.linspace(0, note_duration, n_samples)\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
    "    \n",
    "    #Removing all notes with frequencies <= 9 Hz (includes note 0 = silence)\n",
    "    \n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis = 0)\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples) ** 2\n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo = 160, amplitude = 0.1, sample_rate = 44100, filepath = None):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    if filepath:\n",
    "        from scipy.io import wavfile\n",
    "        samples = (2**15 * samples).astype(np.int16)\n",
    "        wavfile.write(filepath, sample_rate, samples)\n",
    "        return display(Audio(filepath))\n",
    "    else:\n",
    "        return display(Audio(samples, rate = sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's listen to a few chorales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in range(3):\n",
    "    play_chords(train_chorales[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much better and simpler to predict one note at a time. So we will need to preprocess every chorale, turning each chord into an arpegio (i.e., a sequence of notes rather than notes played simultaneuously). So each chorale will be a long sequence of notes (rather than chords), and we can just train a model that can predict the next note given all the previous notes. We will use a sequence-to-sequence approach, where we feed a window to the neural net, and it tries to predict that same window shifted one time step into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also shift the values so that they range from 0 to 46, where 0 represents silence, and values 1 to 46 represent notes 36 (C1) to 81 (A5).\n",
    "\n",
    "And we will train the model on windows of 128 notes (i.e., 32 chords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(batch):\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size = 32, shuffle_buffer_size = None,\n",
    "                window_size = 32, window_shift = 16, cache = True):\n",
    "    \n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "    \n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder = True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "    \n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank = 1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    return dataset.prefetch(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the training set, the validation set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = bach_dataset(train_chorales, shuffle_buffer_size = 1000)\n",
    "valid_set = bach_dataset(valid_chorales)\n",
    "test_set = bach_dataset(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the model:\n",
    "\n",
    "* We could feed the note values directly to the model, as floats, but this would probably not give good results. Indeed, the relationships between notes are not that simple: for example, if you replace a C3 with a C4, the melody will still sound fine, even though these notes are 12 semi-tones apart (i.e., one octave). Conversely, if you replace a C3 with a C\\#3, it's very likely that the chord will sound horrible, despite these notes being just next to each other. So we will use an `Embedding` layer to convert each note to a small vector representation. We will use 5-dimensional embeddings, so the output of this first layer will have a shape of `[batch_size, window_size, 5]`.\n",
    "* We will then feed this data to a small WaveNet-like neural network, composed of a stack of 4 `Conv1D` layers with doubling dilation rates. We will intersperse these layers with `BatchNormalization` layers for faster better convergence.\n",
    "* Then one `LSTM` layer to try to capture long-term patterns.\n",
    "* And finally a `Dense` layer to produce the final note probabilities. It will predict one probability for each chorale in the batch, for each time step, and for each possible note (including silence). So the output shape will be `[batch_size, window_size, 47]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 5)           235       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 32)          352       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 48)          3120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 48)          192       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 96)          12384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 96)          384       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 256)         361472    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 47)          12079     \n",
      "=================================================================\n",
      "Total params: 396,810\n",
      "Trainable params: 396,330\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_embedding_dims = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Embedding(input_dim = n_notes, output_dim = n_embedding_dims,\n",
    "                          input_shape = [None]),\n",
    "    keras.layers.Conv1D(32, kernel_size = 2, padding = \"causal\", activation = \"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(48, kernel_size = 2, padding = \"causal\",\n",
    "                       activation = \"relu\", dilation_rate = 2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(64, kernel_size = 2, padding = \"causal\",\n",
    "                       activation = \"relu\", dilation_rate = 4),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(96, kernel_size = 2, padding = \"causal\",\n",
    "                       activation = \"relu\", dilation_rate = 6),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(256, return_sequences = True),\n",
    "    keras.layers.Dense(n_notes, activation = \"softmax\")\n",
    "    \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 1.8268 - accuracy: 0.5366 - val_loss: 4.4886 - val_accuracy: 0.0451\n",
      "Epoch 2/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.9112 - accuracy: 0.7617 - val_loss: 4.9220 - val_accuracy: 0.0222\n",
      "Epoch 3/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.7682 - accuracy: 0.7909 - val_loss: 3.9601 - val_accuracy: 0.0682\n",
      "Epoch 4/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.6926 - accuracy: 0.8049 - val_loss: 2.8351 - val_accuracy: 0.2065\n",
      "Epoch 5/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.6372 - accuracy: 0.8164 - val_loss: 2.5688 - val_accuracy: 0.3057\n",
      "Epoch 6/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.5930 - accuracy: 0.8250 - val_loss: 1.3880 - val_accuracy: 0.5769\n",
      "Epoch 7/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.5568 - accuracy: 0.8337 - val_loss: 0.7554 - val_accuracy: 0.7773\n",
      "Epoch 8/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.5232 - accuracy: 0.8418 - val_loss: 0.6618 - val_accuracy: 0.8069\n",
      "Epoch 9/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.4937 - accuracy: 0.8493 - val_loss: 0.6435 - val_accuracy: 0.8131\n",
      "Epoch 10/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.4663 - accuracy: 0.8569 - val_loss: 0.6298 - val_accuracy: 0.8184\n",
      "Epoch 11/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.4400 - accuracy: 0.8644 - val_loss: 0.6259 - val_accuracy: 0.8182\n",
      "Epoch 12/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.4171 - accuracy: 0.8703 - val_loss: 0.6966 - val_accuracy: 0.8007\n",
      "Epoch 13/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.4041 - accuracy: 0.8742 - val_loss: 0.8173 - val_accuracy: 0.7672\n",
      "Epoch 14/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.3902 - accuracy: 0.8787 - val_loss: 0.6181 - val_accuracy: 0.8204\n",
      "Epoch 15/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.3575 - accuracy: 0.8890 - val_loss: 0.6559 - val_accuracy: 0.8135\n",
      "Epoch 16/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.3385 - accuracy: 0.8950 - val_loss: 0.6324 - val_accuracy: 0.8186\n",
      "Epoch 17/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.3204 - accuracy: 0.9006 - val_loss: 0.6988 - val_accuracy: 0.8053\n",
      "Epoch 18/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.3068 - accuracy: 0.9045 - val_loss: 0.6543 - val_accuracy: 0.8155\n",
      "Epoch 19/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.2942 - accuracy: 0.9079 - val_loss: 0.6471 - val_accuracy: 0.8176\n",
      "Epoch 20/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.2790 - accuracy: 0.9132 - val_loss: 0.7171 - val_accuracy: 0.8075\n",
      "Epoch 21/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.2660 - accuracy: 0.9171 - val_loss: 0.7644 - val_accuracy: 0.7930\n",
      "Epoch 22/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.2568 - accuracy: 0.9204 - val_loss: 0.6977 - val_accuracy: 0.8087\n",
      "Epoch 23/25\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.2407 - accuracy: 0.9255 - val_loss: 0.7042 - val_accuracy: 0.8092\n",
      "Epoch 24/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.2290 - accuracy: 0.9287 - val_loss: 0.7072 - val_accuracy: 0.8128\n",
      "Epoch 25/25\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.2188 - accuracy: 0.9324 - val_loss: 0.7189 - val_accuracy: 0.8101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21acd062af0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr = 1e-3)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer,\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(train_set, epochs = 25, validation_data = valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 13ms/step - loss: 0.7348 - accuracy: 0.8052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7347909808158875, 0.8052254915237427]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"bach_model.h5\")\n",
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that will generate a new chorale. We will give it a few seed chords, it will convert them to arpegios (the format expected by the model), and use the model to predict the next note, then the next, and so on. In the end, it will group the notes 4 by 4 to create chords again, and return the resulting chorale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale(model, seed_chords, length):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            next_note = model.predict_classes(arpegio)[:1, -1:]\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function, we need some seed chords. Let's use the first 8 chords of one of the test chorales (it's actually just 2 different chords, each played 4 times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[2][:8]\n",
    "play_chords(seed_chords, amplitude = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to generate our first chorale! Let's ask the function to generate 56 more chords, for a total of 64 chords, i.e., 16 bars (assuming 4 chords per bar, i.e., a 4/4 signature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_chorale = generate_chorale(model, seed_chords, 56)\n",
    "play_chords(new_chorale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of always picking the note with the highest score, we will pick the next note randomly, according to the predicted probabilities. For example, if the model predicts a C3 with 75% probability, and a G3 with a 25% probability, then we will pick one of these two notes randomly, with these probabilities. We will also add a temperature parameter that will control how \"hot\" (i.e., daring) we want the system to feel. A high temperature will bring the predicted probabilities closer together, reducing the probability of the likely notes and increasing the probability of the unlikely ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale_v2(model, seed_chords, length, temperature=1):\n",
    "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
    "    arpegio = tf.reshape(arpegio, [1, -1])\n",
    "    for chord in range(length):\n",
    "        for note in range(4):\n",
    "            next_note_probas = model.predict(arpegio)[0, -1:]\n",
    "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
    "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
    "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    return tf.reshape(arpegio, shape=[-1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[2][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\n",
    "play_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\n",
    "play_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\n",
    "play_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play the real chorale and compare our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_chords(test_chorales[2][:64], filepath=\"bach_test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
